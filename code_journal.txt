journal 


=====April 9th ======

I think I need to use the stationary data (perhaps in lagged form?) in the regression models and basically do the same thing for the regression that I did in the time series? 

==pseudo code==
df = stationary_data (maybe lagged stationary data?)
idx = round(len(df) * .8)
train= df[:idx]
test = df[idx:]
run regression models on train. 

result.fit().predict(start=test.index.date[0], end=test.index.date[-1])
mean_squared_error(test, result)


for splitting, should I use 
- sklearn test_train_split
- sklearn TimeSeriesSplit 
- manual 80/20 split?




Random Forest 

Instead of just averaging the prediction of trees, a RF uses two key concepts that give it the name random:

- Random sampling of training observations when building trees
- Random subsets of features for splitting nodes

In other words, Random Forest builds multiple decision trees and merges their predictions together to get a more accurate and stable prediction, rather than relying on individual decision trees.




Made a 2d array of just counts [1,1], [2,2], [3,3]..etc to pass in as X

Made a 2d array of counts and constants [1,1], [2,1], [3,1]... [750,1] to pass in as X

rf requires X be array-like or sparse matrix of shape (n_samples, n_features)

given I only have 1 feature, that should stay constant! 