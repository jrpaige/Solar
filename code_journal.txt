journal 


=====April 9th ======

I think I need to use the stationary data (perhaps in lagged form?) in the regression models and basically do the same thing for the regression that I did in the time series? 

==pseudo code==
df = stationary_data (maybe lagged stationary data?)
idx = round(len(df) * .8)
train= df[:idx]
test = df[idx:]
run regression models on train. 

result.fit().predict(start=test.index.date[0], end=test.index.date[-1])
mean_squared_error(test, result)

For splitting, should I use 
- sklearn test_train_split
- sklearn TimeSeriesSplit 
- manual 80/20 split?



=====April 14th ======

<b> Random Forest </b>
Instead of just averaging the prediction of trees, a RF uses two key concepts that give it the name random:
- Random sampling of training observations when building trees
- Random subsets of features for splitting nodes
In other words, Random Forest builds multiple decision trees and merges their predictions together to get a more accurate and stable prediction, rather than relying on individual decision trees.

Random Forest requires X be array-like or sparse matrix of shape (n_samples, n_features). As such, I made a 2d array of counts and constants [1,1], [2,1], [3,1]... [750,1] to pass in as X. Given I only have 1 feature, that should stay constant. 

Made a lot of progress with the regression models. 

Within the Regression Helper Functions python file see: 
- time_train_test_split()
- multiple_regressors
---------------------------------------------

OLS Lag
The OLS lag model is also called a koyck distributed lag. 
A distributed lag model is a model for time series data in which a regression equation is used to predict current values of a dependent variable based on both the current values of an explanatory variable and the lagged values of this explanatory variable.


=====April 16th ======

Completed smf.ols model and sm.OLS linear model 

current outputs

   ---- MSE Scores ----  
Random Forest Regressor   0.03131
Linear Regression         0.01594
Bagging Regressor         0.03347
AdaBoost Regressor        0.01464
smf ols                   0.01356
sm OLS Linear             0.01494


Created Model_Notebook.ipynb to run clean code of the final models 
TSA.py is a collection of previous code of now un-used code that I wish to save.


=====April 17th ======
try 2-4 lags



validate 


created 6 different regressors

Random Forest, Linear, Bagging, AdaBoost, OLS Linear, OLS

Bagging and Adaboost aren't super 



