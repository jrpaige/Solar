journal 


=====April 9th ======

I think I need to use the stationary data (perhaps in lagged form?) in the regression models and basically do the same thing for the regression that I did in the time series? 

==pseudo code==
df = stationary_data (maybe lagged stationary data?)
idx = round(len(df) * .8)
train= df[:idx]
test = df[idx:]
run regression models on train. 

result.fit().predict(start=test.index.date[0], end=test.index.date[-1])
mean_squared_error(test, result)

For splitting, should I use 
- sklearn test_train_split
- sklearn TimeSeriesSplit 
- manual 80/20 split?



=====April 14th ======

<b> Random Forest </b>
Instead of just averaging the prediction of trees, a RF uses two key concepts that give it the name random:
- Random sampling of training observations when building trees
- Random subsets of features for splitting nodes
In other words, Random Forest builds multiple decision trees and merges their predictions together to get a more accurate and stable prediction, rather than relying on individual decision trees.

Random Forest requires X be array-like or sparse matrix of shape (n_samples, n_features). As such, I made a 2d array of counts and constants [1,1], [2,1], [3,1]... [750,1] to pass in as X. Given I only have 1 feature, that should stay constant. 

Made a lot of progress with the regression models. 

Within the Regression Helper Functions python file see: 
- time_train_test_split()
- multiple_regressors
---------------------------------------------

OLS Lag
The OLS lag model is also called a koyck distributed lag. 
A distributed lag model is a model for time series data in which a regression equation is used to predict current values of a dependent variable based on both the current values of an explanatory variable and the lagged values of this explanatory variable.


=====April 16th ======

Completed smf.ols model and sm.OLS linear model 

current outputs

   ---- MSE Scores ----  
Random Forest Regressor   0.03131
Linear Regression         0.01594
Bagging Regressor         0.03347
AdaBoost Regressor        0.01464
smf ols                   0.01356
sm OLS Linear             0.01494


Created Model_Notebook.ipynb to run clean code of the final models 
TSA.py is a collection of previous code of now un-used code that I wish to save.


=====April 17th ======


Created 6 different regressors
Random Forest, Linear, Bagging, AdaBoost, OLS Linear, OLS
Bagging and Adaboost aren't super helpful, so pulling those. 
Between Linear and OLS Linear, OLS Linear performs better, so pulling Linear from the group as well.
The original 3 will remain


try different lags:

rf, ols_lin, ols = multiple_regressors(diff, lag_len=#)

      ---- MSE Scores ----     
[2 LAGS] 
Random Forest Regressor   0.02131
sm OLS Linear             0.01342
smf ols                   0.01348

[3 LAGS]
Random Forest Regressor   0.01708
sm OLS Linear             0.01281
smf ols                   0.01348

[4 LAGS]
Random Forest Regressor   0.01554
sm OLS Linear             0.01245
smf ols                   0.01348

[5 LAGS]
Random Forest Regressor   0.01485
sm OLS Linear             0.01327
smf ols                   0.01348

[RESULTS]
smf ols mse score stays the same regardless of # of lags

fixed smf ols to ensure formula was updating with number of lags.
smf ols mse score now updates

[UPDATED]
2 lags doesn't perform well

[3 LAGS] avg = 0.0145
Random Forest Regressor   0.01728
sm OLS Linear             0.01281
smf ols                   0.01348

[4 LAGS] avg = 0.0138
Random Forest Regressor   0.01546
sm OLS Linear             0.01245
smf ols                   0.01354

[5 LAGS] avg = 0.0141
Random Forest Regressor   0.01473
sm OLS Linear             0.01327
smf ols                   0.01419

RF performs best w/ 5 
OLS linear performs best w/ 4
smf ols performs best w/ 3

based on averages, 4 lags scores best 
will change defaults to 4 lags