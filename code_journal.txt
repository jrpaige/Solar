journal 


=====April 9th ======

I think I need to use the stationary data (perhaps in lagged form?) in the regression models and basically do the same thing for the regression that I did in the time series? 

==pseudo code==
df = stationary_data (maybe lagged stationary data?)
idx = round(len(df) * .8)
train= df[:idx]
test = df[idx:]
run regression models on train. 

result.fit().predict(start=test.index.date[0], end=test.index.date[-1])
mean_squared_error(test, result)

For splitting, should I use 
- sklearn test_train_split
- sklearn TimeSeriesSplit 
- manual 80/20 split?



=====April 14th ======

<b> Random Forest </b>
Instead of just averaging the prediction of trees, a RF uses two key concepts that give it the name random:
- Random sampling of training observations when building trees
- Random subsets of features for splitting nodes
In other words, Random Forest builds multiple decision trees and merges their predictions together to get a more accurate and stable prediction, rather than relying on individual decision trees.

Random Forest requires X be array-like or sparse matrix of shape (n_samples, n_features). As such, I made a 2d array of counts and constants [1,1], [2,1], [3,1]... [750,1] to pass in as X. Given I only have 1 feature, that should stay constant. 

Made a lot of progress with the regression models. 

Within the Regression Helper Functions python file see: 
- time_train_test_split()
- multiple_regressors
---------------------------------------------

OLS Lag
The OLS lag model is also called a koyck distributed lag. 
A distributed lag model is a model for time series data in which a regression equation is used to predict current values of a dependent variable based on both the current values of an explanatory variable and the lagged values of this explanatory variable.


=====April 16th ======

Completed smf.ols model and sm.OLS linear model 

current outputs

   ---- MSE Scores ----  
Random Forest Regressor   0.03131
Linear Regression         0.01594
Bagging Regressor         0.03347
AdaBoost Regressor        0.01464
smf ols                   0.01356
sm OLS Linear             0.01494


Created Model_Notebook.ipynb to run clean code of the final models 
TSA.py is a collection of previous code of now un-used code that I wish to save.


=====April 17th ======


Created 6 different regressors
Random Forest, Linear, Bagging, AdaBoost, OLS Linear, OLS
Bagging and Adaboost aren't super helpful, so pulling those. 
Between Linear and OLS Linear, OLS Linear performs better, so pulling Linear from the group as well.
The original 3 will remain


try different lags:

rf, ols_lin, ols = multiple_regressors(diff, lag_len=#)

      ---- MSE Scores ----     
[2 LAGS] 
Random Forest Regressor   0.02131
sm OLS Linear             0.01342
smf ols                   0.01348

[3 LAGS]
Random Forest Regressor   0.01708
sm OLS Linear             0.01281
smf ols                   0.01348

[4 LAGS]
Random Forest Regressor   0.01554
sm OLS Linear             0.01245
smf ols                   0.01348

[5 LAGS]
Random Forest Regressor   0.01485
sm OLS Linear             0.01327
smf ols                   0.01348

[RESULTS]
smf ols mse score stays the same regardless of # of lags

fixed smf ols to ensure formula was updating with number of lags.
smf ols mse score now updates

[UPDATED]
2 lags doesn't perform well

[3 LAGS] avg = 0.0145
Random Forest Regressor   0.01728
sm OLS Linear             0.01281
smf ols                   0.01348

[4 LAGS] avg = 0.0138
Random Forest Regressor   0.01546
sm OLS Linear             0.01245
smf ols                   0.01354

[5 LAGS] avg = 0.0141
Random Forest Regressor   0.01473
sm OLS Linear             0.01327
smf ols                   0.01419

RF performs best w/ 5 
OLS linear performs best w/ 4
smf ols performs best w/ 3

based on averages, 4 lags scores best 
will change defaults to 4 lags


APRIL 24th 


To Do:

- Add some training data to plots
- Add info into plot titles
- Go through annomoloy detection 
    - think about the 2016 increase due to Trump
- Look into the beta coefs for ols 
    - how the previous time frames affect future time frames
- Remove ARMA from TimeSeriesHelper Funcs, no need




evaluate arima model is only testing first prediction against test data to provide mse score. 

not helpful. 
prefer auto_arima_pdq(train, trace_list=True)
although it does not print out mse score 


AR in ARIMA means autoregressive ; linear regression uses its own lags as predictors

p = AR number of lags to be used as predictors 
q = order of the moving average (MA) term = 
    number of lagged forecast errors  
d=  minimum number of differencing to make stationary
    number of lags 

If time series is already stationary, d=0

!!!try passing 1 into d on basic un-differenced data

Y{t-1} : is the lag1 of the series
beta1  : is the coefficient of lag1 that the model estimates 
alpha  : is the intercept term, also estimated by the model

pure MA model only depends on lagged forcast errors  
error terms are the errors of the autoregressive models of the respective lags 
Errors Et and A(t-1) are the errors from 

AR: Y{t} = β{1}Y{t-1} + β{2}T{t-2} +... β{0}Y{0} + ∈{t}
MA: Y{t-1} = β{1}Y{t-2} + β{2}T{t-3} +... β{0}Y{0} + ∈{t-1}

beta coefficients are the lags? 

ARIMA ==
Y{t} = α + β{1}Y{t-1} + β{2}Y{t-2} + .. β{p}Y{t-p}∈{t} + Φ{1}∈{t-1} + Φ{2}∈{t-2}+ .. Φ{q}∈{t-q}

Predicted Yt = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags) 
                            
                            
Predicted Yt                   Y{t}
=                              = 
Constant                       α
+                              + 
Linear combination Lags of Y   β{1}Y{t-1} + β{2}Y{t-2}
(upto p lags)                  β{p}Y{t-p}∈{t}
+                              + 
Linear combination of 
    Lagged forecast errors     Φ{1}∈{t-1} + Φ{2}∈{t-2}
(upto q lags)                  Φ{q}∈{t-q}                        



Partial autocoreelation of lag k of a series is the cofficient of that lag in the autoregressive equation of Y

Yt = alpha0 + alpha1 Y{t-1} + alpha2 Y{t-2} + alpha3 Y{t-3}
Y_t = current series 
Y_t-1 is the lag 1 of Y 
partial autocorrelation of lag 3 (Y_t-3) is the coefficient alpha_3 of Y_t-3


## Created Time_Series_Dilligence to go through basic steps of ARIMA so this is separate from the Model_Notebook. 

